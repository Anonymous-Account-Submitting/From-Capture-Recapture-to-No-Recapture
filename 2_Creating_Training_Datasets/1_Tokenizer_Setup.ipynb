{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "439d2377",
   "metadata": {},
   "source": [
    "# This file simply takes into account all instructions found in every program of the experiments and setsup a tokenizer for it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d638496d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import train_test_split\n",
    "from matplotlib import pyplot as plt\n",
    "import random\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import timeit\n",
    "from sklearn import metrics\n",
    "import scipy.io as sio\n",
    "import csv\n",
    "import os\n",
    "import math \n",
    "import random as rand\n",
    "from numpy import genfromtxt\n",
    "from numpy import sum,isrealobj,sqrt\n",
    "from numpy.random import standard_normal\n",
    "\n",
    "#import torch\n",
    "#from torch import nn\n",
    "\n",
    "import warnings\n",
    "from scipy.spatial import distance\n",
    "\n",
    "\n",
    "from importlib import reload\n",
    "import sys\n",
    "sys.path.append(\"../Imports/\") # Adds higher directory to python modules path.\n",
    "\n",
    "from Modules.Tools import MapTool\n",
    "from Modules.Tools import BoxPlot\n",
    "from Modules.Tools import Extractor\n",
    "from Modules.Tools import peakCorrelation as pC"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ec13075",
   "metadata": {},
   "source": [
    "# Notebook Parameters\n",
    "- this is simply for obtaining the graphs with manual mapping. Pad is closely relate to the sampling rate for the EM signal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0a054b5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_folder = \"../Datasets/Instruction_Code/\"\n",
    "save_folder = \"../Datasets/Tokenizer/\"\n",
    "pad = 8\n",
    "y_low=-2.8\n",
    "y_high=2.8\n",
    "amount=3000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e43dad41",
   "metadata": {},
   "source": [
    "# Tokenizer Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3546b8da",
   "metadata": {},
   "source": [
    "We need to be able to convert code from either string or a code document into a computer/model readable format.\n",
    "This can create several challenges.\n",
    "- The amount of code can vary between programs.\n",
    "- The signal size can vary.\n",
    "- The embedded instructions sould be unique like bagging words.\n",
    "- Note that words for each instruction mustnt be spaced, hypened, unscored or contain any unique lettering or symbol as this will split the word/instruction into different tokens.\n",
    "\n",
    "Thus we will limit the signal size and amount of instructions to a set amount to generate the program in sections for all signals.\n",
    "Consequently, we will create a precise convertion from code to numbers, where each number will represent a unique instruction (i.e., tokenization).\n",
    "\n",
    "Later work my switch from tokenizer to something else, however, tokenizer is a simple method that fullfills the requirments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "256d18a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "\n",
    "tokenizer = Tokenizer(num_words=100, char_level=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c1de6aca",
   "metadata": {},
   "outputs": [],
   "source": [
    "Base_Benign_Code = np.load(load_folder + \"Base_Benign.npy\").tolist()\n",
    "Base_Anom_Code = np.load(load_folder + \"Base_Anom.npy\").tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f48cb18f",
   "metadata": {},
   "outputs": [],
   "source": [
    "Update_50_A_Benign_Code = np.load(load_folder + \"Update_50_A_Benign.npy\").tolist()\n",
    "Update_50_B_Benign_Code = np.load(load_folder + \"Update_50_B_Benign.npy\").tolist()\n",
    "Update_50_C_Benign_Code = np.load(load_folder + \"Update_50_C_Benign.npy\").tolist()\n",
    "\n",
    "Update_50_A_Anom_Code = np.load(load_folder + \"Update_50_A_Anom.npy\").tolist()\n",
    "Update_50_B_Anom_Code = np.load(load_folder + \"Update_50_B_Anom.npy\").tolist()\n",
    "Update_50_C_Anom_Code = np.load(load_folder + \"Update_50_C_Anom.npy\").tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "00972309",
   "metadata": {},
   "outputs": [],
   "source": [
    "Update_75_A_Benign_Code = np.load(load_folder + \"Update_75_A_Benign.npy\").tolist()\n",
    "Update_75_B_Benign_Code = np.load(load_folder + \"Update_75_B_Benign.npy\").tolist()\n",
    "Update_75_C_Benign_Code = np.load(load_folder + \"Update_75_C_Benign.npy\").tolist()\n",
    "\n",
    "Update_75_A_Anom_Code = np.load(load_folder + \"Update_75_A_Anom.npy\").tolist()\n",
    "Update_75_B_Anom_Code = np.load(load_folder + \"Update_75_B_Anom.npy\").tolist()\n",
    "Update_75_C_Anom_Code = np.load(load_folder + \"Update_75_C_Anom.npy\").tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ecba0100",
   "metadata": {},
   "outputs": [],
   "source": [
    "Update_100_A_Benign_Code = np.load(load_folder + \"Update_100_A_Benign.npy\").tolist()\n",
    "Update_100_B_Benign_Code = np.load(load_folder + \"Update_100_B_Benign.npy\").tolist()\n",
    "Update_100_C_Benign_Code = np.load(load_folder + \"Update_100_C_Benign.npy\").tolist()\n",
    "\n",
    "Update_100_A_Anom_Code = np.load(load_folder + \"Update_100_A_Anom.npy\").tolist()\n",
    "Update_100_B_Anom_Code = np.load(load_folder + \"Update_100_B_Anom.npy\").tolist()\n",
    "Update_100_C_Anom_Code = np.load(load_folder + \"Update_100_C_Anom.npy\").tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d8e87cce",
   "metadata": {},
   "outputs": [],
   "source": [
    "Train_Program_1_Code = np.load(load_folder + \"Train_Program_1.npy\").tolist()\n",
    "Train_Program_2_Code = np.load(load_folder + \"Train_Program_2.npy\").tolist()\n",
    "Train_Program_3_Code = np.load(load_folder + \"Train_Program_3.npy\").tolist()\n",
    "Train_Program_4_Code = np.load(load_folder + \"Train_Program_4.npy\").tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b6df8e6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_code = np.copy(Base_Benign_Code)\n",
    "all_code = np.concatenate((all_code, Base_Anom_Code))\n",
    "\n",
    "all_code = np.concatenate((all_code, Update_50_A_Benign_Code))\n",
    "all_code = np.concatenate((all_code, Update_50_B_Benign_Code))\n",
    "all_code = np.concatenate((all_code, Update_50_C_Benign_Code))\n",
    "all_code = np.concatenate((all_code, Update_50_A_Anom_Code))\n",
    "all_code = np.concatenate((all_code, Update_50_B_Anom_Code))\n",
    "all_code = np.concatenate((all_code, Update_50_C_Anom_Code))\n",
    "\n",
    "all_code = np.concatenate((all_code, Update_75_A_Benign_Code))\n",
    "all_code = np.concatenate((all_code, Update_75_B_Benign_Code))\n",
    "all_code = np.concatenate((all_code, Update_75_C_Benign_Code))\n",
    "all_code = np.concatenate((all_code, Update_75_A_Anom_Code))\n",
    "all_code = np.concatenate((all_code, Update_75_B_Anom_Code))\n",
    "all_code = np.concatenate((all_code, Update_75_C_Anom_Code))\n",
    "\n",
    "all_code = np.concatenate((all_code, Update_100_A_Benign_Code))\n",
    "all_code = np.concatenate((all_code, Update_100_B_Benign_Code))\n",
    "all_code = np.concatenate((all_code, Update_100_C_Benign_Code))\n",
    "all_code = np.concatenate((all_code, Update_100_A_Anom_Code))\n",
    "all_code = np.concatenate((all_code, Update_100_B_Anom_Code))\n",
    "all_code = np.concatenate((all_code, Update_100_C_Anom_Code))\n",
    "\n",
    "all_code = np.concatenate((all_code, Train_Program_1_Code))\n",
    "all_code = np.concatenate((all_code, Train_Program_2_Code))\n",
    "all_code = np.concatenate((all_code, Train_Program_3_Code))\n",
    "all_code = np.concatenate((all_code, Train_Program_4_Code))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "631b09a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['adc' 'add' 'brcc-f' 'brcc-t' 'brlo-f' 'brlo-t' 'clr' 'cp' 'cpi' 'dec'\n",
      " 'inc' 'ldi' 'mov' 'mul' 'rjmp' 'sbc' 'sub']\n"
     ]
    }
   ],
   "source": [
    "all_code_unique = np.unique(all_code)\n",
    "print(all_code_unique)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aca14c1f",
   "metadata": {},
   "source": [
    "# Next we need to remove hyphins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a49764b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(all_code_unique)):\n",
    "    if \"-t\" in all_code_unique[i]:\n",
    "        all_code_unique[i] = all_code_unique[i][:-2] + \"T\"\n",
    "    elif \"-f\" in all_code_unique[i]:\n",
    "        all_code_unique[i] = all_code_unique[i][:-2] + \"F\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9863f8dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['adc' 'add' 'brccF' 'brccT' 'brloF' 'brloT' 'clr' 'cp' 'cpi' 'dec' 'inc'\n",
      " 'ldi' 'mov' 'mul' 'rjmp' 'sbc' 'sub']\n"
     ]
    }
   ],
   "source": [
    "print(all_code_unique)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3396d9a1",
   "metadata": {},
   "source": [
    "# Next, for tokenizer to work, we need it in a single string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c99ecf3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adc, add, brccF, brccT, brloF, brloT, clr, cp, cpi, dec, inc, ldi, mov, mul, rjmp, sbc, sub\n"
     ]
    }
   ],
   "source": [
    "string_all_code_unique = \"\"\n",
    "First = True\n",
    "for inst in all_code_unique:\n",
    "    if First:\n",
    "        string_all_code_unique = string_all_code_unique + inst\n",
    "        First = False\n",
    "    else:\n",
    "        string_all_code_unique = string_all_code_unique + \", \" + inst\n",
    "print(string_all_code_unique)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b3e69bc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = np.array([string_all_code_unique])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "acc33591",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(save_folder + \"Code_for_Tokenizer.npy\", temp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d9b8348",
   "metadata": {},
   "source": [
    "# Next we tokenize the string to obtain the tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ad0d6b9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_instructions = string_all_code_unique\n",
    "\n",
    "sentence_Code = [all_instructions]\n",
    "tokenizer.fit_on_texts(sentence_Code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5e4c389b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17]]\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.texts_to_sequences(sentence_Code))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d6136892",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[5]]\n",
      "[[12]]\n",
      "[[1]]\n",
      "[[16]]\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.texts_to_sequences(['brloF']))\n",
    "print(tokenizer.texts_to_sequences(['ldi']))\n",
    "print(tokenizer.texts_to_sequences(['adc']))\n",
    "print(tokenizer.texts_to_sequences(['sbc']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c23f7e38",
   "metadata": {},
   "source": [
    "# Now we only need to load the Code_for_Tokenizer.npy, and tokenize.fit_on_text([Code_for_Tokenizer]) to obtain the exact same tokenizer."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
